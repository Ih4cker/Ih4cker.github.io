<!DOCTYPE html>
<html lang="zh-CN">
  <head><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="关于Python爬虫种类、法律、轮子的一二三"/><meta name="keywords" content="python爬虫, API漫步者" /><link rel="alternate" href="/atom.xml" title="API漫步者" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.11.0" />
<link rel="canonical" href="http://www.1991.site/2020/11/16/关于Python爬虫种类、法律、轮子的一二三/"/>

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" /><script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0" />

<script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "OhR8HqNS6dEph2ggaBF8npi8-gzGzoHsz",
      appKey: "ohkLWE1SEJGE0robAWkxnhx8"
    });
  </script><script>
  window.config = {"leancloud":{"app_id":"OhR8HqNS6dEph2ggaBF8npi8-gzGzoHsz","app_key":"ohkLWE1SEJGE0robAWkxnhx8"},"toc":true,"fancybox":true,"pjax":"","latex":true};
</script>

    <title>关于Python爬虫种类、法律、轮子的一二三 - API漫步者</title>
  <meta name="generator" content="Hexo 5.3.0"></head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">API漫步者</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/archives/">
        <li class="mobile-menu-item">归档
          </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">API漫步者</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            归档
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/categories/">
            分类
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">关于Python爬虫种类、法律、轮子的一二三
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2020-11-16
        </span><span class="post-category">
            <a href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
            <a href="/categories/%E7%88%AC%E8%99%AB/python3/">python3</a>
            </span>
        <span class="post-visits"
             data-url="/2020/11/16/%E5%85%B3%E4%BA%8EPython%E7%88%AC%E8%99%AB%E7%A7%8D%E7%B1%BB%E3%80%81%E6%B3%95%E5%BE%8B%E3%80%81%E8%BD%AE%E5%AD%90%E7%9A%84%E4%B8%80%E4%BA%8C%E4%B8%89/"
             data-title="关于Python爬虫种类、法律、轮子的一二三">
          阅读次数 0
        </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Welcome-to-the-D-age"><span class="toc-text">Welcome to the D-age</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%95%E5%BE%8B%E7%9A%84%E8%BE%B9%E7%95%8C-%E6%8A%80%E6%9C%AF%E6%97%A0%E7%BD%AA"><span class="toc-text">法律的边界,技术无罪</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%A5%E6%AC%B2%E5%96%84%E5%85%B6%E4%BA%8B"><span class="toc-text">工欲善其事</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="toc-text">同步爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%88%AC%E8%99%AB"><span class="toc-text">多进程爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB"><span class="toc-text">多线程爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B%E7%88%AC%E8%99%AB"><span class="toc-text">异步协程爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E7%88%AC%E8%99%AB"><span class="toc-text">多进程 + 多线程 爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B-%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B-%E7%88%AC%E8%99%AB"><span class="toc-text">多进程 + 异步协程 爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E9%87%87%E9%9B%86"><span class="toc-text">分布式采集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AE%E5%AD%90%E4%BB%AC%EF%BC%8C%E4%BD%A0%E4%BB%AC%E8%BE%9B%E8%8B%A6%E4%BA%86"><span class="toc-text">轮子们，你们辛苦了</span></a></li></ol>
    </div>
  </div><div class="post-content"><h2 id="Welcome-to-the-D-age"><a href="#Welcome-to-the-D-age" class="headerlink" title="Welcome to the D-age"></a>Welcome to the D-age</h2><p>对于网络上的公开数据，理论上只要由服务端发送到前端都可以由爬虫获取到。但是Data-age时代的到来，数据是新的黄金，毫不夸张的说，数据是未来的一切。基于统计学数学模型的各种人工智能的出现，离不开数据驱动。数据采集、清洗是最末端的技术成本，网络爬虫也是基础采集脚本。但是有几个值得关注的是：</p>
<ul>
<li>对于实时变化的网络环境，爬虫的持续有效性如何保证</li>
<li>数据采集、清洗规则的适用范围</li>
<li>数据采集的时间与质量–效率</li>
<li>爬与反爬的恩怨</li>
<li>爬虫的法律界限</li>
</ul>
<a id="more"></a> 

<h2 id="法律的边界-技术无罪"><a href="#法律的边界-技术无罪" class="headerlink" title="法律的边界,技术无罪"></a>法律的边界,技术无罪</h2><p>对于上面几个关注点，我最先关注的便是爬虫的<strong>法律界限</strong> ，我曾经咨询过一个律师：</p>
<blockquote>
<p>Q: 老师，我如果用爬虫爬取今日头条这种类型网站的千万级公开数据，算不算违法呢？<br>A: 爬取的公开数据不得进行非法使用或者商业利用</p>
</blockquote>
<p>简单的概括便是爬虫爬取的数据如果进行商业出售或者有获利的使用，便构成了“非法使用”。而一般的爬虫程序并不违法，其实这是从法律专业的一方来解读，如果加上技术层面的维度，那么应该从这几方面考虑：</p>
<blockquote>
<ul>
<li>爬取的数据量</li>
<li>爬取数据的类型（数据具有巨大的商业价值，未经对方许可，任何人不得非法获取其数据并用于**<em>经营行为**</em>）</li>
<li>爬取的数据用途 (同行竞争？出售？经营？分析？实验？…)</li>
<li>是否遵循网站的robots.txt 即 机器人协议</li>
<li>爬取行为是否会对对方网站造成不能承受的损失（大量的爬取请求会把一个小型网站拖垮）</li>
</ul>
</blockquote>
<p>其实爬虫构成犯罪的案例是开始增多的，相关新闻:</p>
<blockquote>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.sohu.com/a/256579233_161795">当爬虫遇上法律会有什么风险？</a></li>
<li><a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1609682215455337498&wfr=spider&for=pc">程序员爬虫竟构成犯罪？</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/nick477931661/p/9139137.html">爬虫相关法律知识</a></li>
</ol>
</blockquote>
<p>如果你的上级或公司要求你爬取某些网站的大量公开数据，你会怎么办呢？可以参考第2条新闻。法律矛盾点关键在于前面考虑的前三点，如果是个人隐私数据，是不能爬取的，如果是非公开数据，是不能爬取的，而对于其他大量的公开数据爬取，看人家查不查的到你，要不要起诉你。技术在你的手上，非法与否在于你怎么去用。最好的爬取道德原则是：</p>
<blockquote>
<ul>
<li>减少并发请求</li>
<li>延长请求间隔</li>
<li>不进行公开出售数据</li>
<li>遵循网站 robots协议</li>
</ul>
</blockquote>
<p>当然，反爬最有效的便(目的均在于拦截爬虫进入网站数据范围)是:</p>
<blockquote>
<ul>
<li>要求用户密码+验证码</li>
<li>加密数据</li>
<li>js混淆</li>
<li>css混淆</li>
<li>针对IP请求频率封锁</li>
<li>针对cookie、session单个账户请求频率封锁单日请求次数</li>
<li>对关键数据进行拆分合并</li>
<li>对爬虫投毒（返回假数据）</li>
<li>完善robots.txt</li>
<li>识别点击九宫图中没有包含xxx的图片等（终极验证码)</li>
<li>设置黑白名单、IP用户组等</li>
</ul>
</blockquote>
<h2 id="工欲善其事"><a href="#工欲善其事" class="headerlink" title="工欲善其事"></a>工欲善其事</h2><p>针对网站的公开数据进行爬取，我们一般都要先对网站数据进行<strong>分析，定位</strong>，以确定其采集规则，如果网站设置了访问权限，那么便不属于我们的爬虫采集范围了<i class="fa fa-android"></i>:)<br>分析好采集规则，写好了采集数据持久化（存入数据库、导出为word、excel、csv、下载等）的相关代码，整个爬虫运行正常。那么怎样才能提高采集速度呢？</p>
<blockquote>
<ul>
<li>多进程采集</li>
<li>多线程采集</li>
<li>异步协程采集</li>
<li>多进程 + 多线程采集 </li>
<li>多进程 + 异步协程采集</li>
<li>分布式采集</li>
</ul>
</blockquote>
<p>异步爬虫是同步爬虫的升级版，在同步爬虫中，无论你怎么优化代码，同步IO的阻塞是最大的致命伤。同步阻塞会让采集任务一个个排着长队领票等待执行。而异步采集不会造成IO阻塞，充分利用了IO阻塞任务的等待时间去执行其他任务。</p>
<blockquote>
<p>在IO 模型中，只有IO多路复用（I/O  multiplexing）{在内核处理IO请求结果为可读或可写时调用回调函数} 不阻塞 “内核拷贝IO请求数据到用户空间”这个过程，实现异步IO操作。</p>
</blockquote>
<h3 id="同步爬虫"><a href="#同步爬虫" class="headerlink" title="同步爬虫"></a>同步爬虫</h3><p>一般的同步爬虫，我们可以写一个，（以爬取<a target="_blank" rel="noopener" href="http://www.quanjing.com/creative/SearchCreative.aspx?id=7">图片网站</a>图片为例），我们来看看其下载该网址所有图片所花费的时间：</p>
<blockquote>
<p>以下代码为后面多个例程的共同代码:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> urllib.request <span class="keyword">as</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment">#目标网址</span></span><br><span class="line">url = <span class="string">&#x27;http://www.quanjing.com/creative/SearchCreative.aspx?id=7&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_one_pic</span>(<span class="params">url:<span class="built_in">str</span>,name:<span class="built_in">str</span>,suffix:<span class="built_in">str</span>=<span class="string">&#x27;jpg&#x27;</span></span>):</span></span><br><span class="line">	<span class="comment">#下载单张图片</span></span><br><span class="line">    path = <span class="string">&#x27;.&#x27;</span>.join([name,suffix])</span><br><span class="line">    response = request.urlopen(url)</span><br><span class="line">    wb_data = response.read()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(wb_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_many_pic</span>(<span class="params">urls:<span class="built_in">list</span></span>):</span></span><br><span class="line">	<span class="comment">#下载多张图片</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> urls:</span><br><span class="line">        ts = <span class="built_in">str</span>(<span class="built_in">int</span>(time.time() * <span class="number">1000</span>))</span><br><span class="line">        download_one_pic(i, ts)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">u&#x27;下载完成,%d张图片,耗时:%.2fs&#x27;</span> % (<span class="built_in">len</span>(urls), (end - start)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pic_urls</span>(<span class="params">url:<span class="built_in">str</span></span>)-&gt;list:</span></span><br><span class="line">	<span class="comment">#获取页面所有图片链接</span></span><br><span class="line">    response = request.urlopen(url)</span><br><span class="line">    wb_data = response.read()</span><br><span class="line">    html = etree.HTML(wb_data)</span><br><span class="line">    pic_urls = html.xpath(<span class="string">&#x27;//a[@class=&quot;item lazy&quot;]/img/@src&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> pic_urls</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allot</span>(<span class="params">pic_urls:<span class="built_in">list</span>,n:<span class="built_in">int</span></span>)-&gt;list:</span></span><br><span class="line">    <span class="comment">#根据给定的组数，分配url给每一组</span></span><br><span class="line">    _len = <span class="built_in">len</span>(pic_urls)</span><br><span class="line">    base = <span class="built_in">int</span>(_len / n)</span><br><span class="line">    remainder = _len % n</span><br><span class="line">    groups = [pic_urls[i * base:(i + <span class="number">1</span>) * base] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    remaind_group = pic_urls[n * base:]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(remainder):</span><br><span class="line">        groups[i].append(remaind_group[i])</span><br><span class="line">    <span class="keyword">return</span> [i <span class="keyword">for</span> i <span class="keyword">in</span> groups <span class="keyword">if</span> i]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>同步爬虫:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawler</span>():</span></span><br><span class="line">	<span class="comment">#同步下载</span></span><br><span class="line">    pic_urls = get_pic_urls(url)</span><br><span class="line">    download_many_pic(pic_urls)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>执行同步爬虫，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crawler()</span><br></pre></td></tr></table></figure>
<p>输出（时间可能不一样，取决于你的网速）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下载完成,<span class="number">196</span>张图片,耗时:<span class="number">49.04</span>s</span><br></pre></td></tr></table></figure>
<p>在同一个网络环境下，排除网速时好时坏，可以下载多几次取平均下载时间，在我的网络环境下，我下载了5次，平均耗时约55.26s</p>
<h3 id="多进程爬虫"><a href="#多进程爬虫" class="headerlink" title="多进程爬虫"></a>多进程爬虫</h3><p>所以为了提高采集速度，我们可以写一个多进程爬虫（以爬取<a target="_blank" rel="noopener" href="http://www.quanjing.com/creative/SearchCreative.aspx?id=7">图片网站</a>图片为例）:<br>为了对应多进程的进程数n，我们可以将图片链接列表分成n组，多进程爬虫:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing.pool <span class="keyword">import</span> Pool</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiprocess_crawler</span>(<span class="params">processors:<span class="built_in">int</span></span>):</span></span><br><span class="line">    <span class="comment">#多进程爬虫</span></span><br><span class="line">    pool = Pool(processors)</span><br><span class="line">    pic_urls = get_pic_src(url)</span><br><span class="line">    <span class="comment">#对应多进程的进程数processors，我们可以将图片链接列表分成processors组</span></span><br><span class="line">    url_groups = allot(pic_urls,processors)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> url_groups:</span><br><span class="line">        pool.apply_async(func=download_many_pic,args=(i,))</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>执行爬虫,进程数设为4，一般是cpu数量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">multiprocess_crawler(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">18.22</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">18.99</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">18.97</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">19.51</span>s</span><br></pre></td></tr></table></figure>
<p>可以看出，多进程比原先的同步爬虫快许多，整个程序耗时19.51s，为什么不是同步爬虫的55s/4 ≈ 14s呢？因为进程间的切换需要耗时。<br>如果把进程数增大，那么:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">进程数:<span class="number">10</span> , 耗时：<span class="number">12.3</span>s</span><br><span class="line">进程数:<span class="number">30</span> , 耗时：<span class="number">2.81</span>s</span><br><span class="line">进程数:<span class="number">40</span> , 耗时：<span class="number">11.34</span>s</span><br></pre></td></tr></table></figure>
<p>对于多进程爬虫来说，虽然实现异步爬取，但也不是越多进程越好，进程间切换的开销不仅会让你崩溃，有时还会让你的程序崩溃。一般用进程池Pool维护，Pool的processors设为CPU数量。进程的数量设置超过100个便让我的程序崩溃退出。使用进程池可以保证当前在跑的进程数量控制为设置的数量，只有池子没满才能加新的进程进去。</p>
<h3 id="多线程爬虫"><a href="#多线程爬虫" class="headerlink" title="多线程爬虫"></a>多线程爬虫</h3><p>多线程版本可以在单进程下进行异步采集，但线程间的切换开销也会随着线程数的增大而增大。当线程间需要共享变量内存时，此时会有许多不可预知的变量读写操作发生，python为了使线程同步，给每个线程共享变量加了全局解释器锁GIL。而我们的爬虫不需要共享变量，因此是线程安全的，不用加锁。多线程版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_multithread_crawler</span>(<span class="params">pic_urls:<span class="built_in">list</span>,threads:<span class="built_in">int</span></span>):</span></span><br><span class="line">    begin = <span class="number">0</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        _threads = []</span><br><span class="line">        urls = pic_urls[begin:begin+threads]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> urls:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> urls:</span><br><span class="line">            ts = <span class="built_in">str</span>(<span class="built_in">int</span>(time.time()*<span class="number">10000</span>))+<span class="built_in">str</span>(random.randint(<span class="number">1</span>,<span class="number">100000</span>))</span><br><span class="line">            t = Thread(target=download_one_pic,args=(i,ts))</span><br><span class="line">            _threads.append(t)</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> _threads:</span><br><span class="line">            t.setDaemon(<span class="literal">True</span>)</span><br><span class="line">            t.start()</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> _threads:</span><br><span class="line">            t.join()</span><br><span class="line">        begin += threads</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">u&#x27;下载完成,%d张图片,耗时:%.2fs&#x27;</span> % (<span class="built_in">len</span>(pic_urls), (end - start)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multithread_crawler</span>(<span class="params">threads:<span class="built_in">int</span></span>):</span></span><br><span class="line">    pic_urls = get_pic_src(url)</span><br><span class="line">    run_multithread_crawler(pic_urls,threads)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>并发线程数太多会让我们的系统开销越大，使程序花费时间越长，同时也会增大目标网站识别爬虫机器行为的几率。因此设置好一个适当的线程数以及爬取间隔是良好的爬虫习惯。<br>执行多线程爬虫，设置线程数为50</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">multithreads_crawler(<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下载完成,<span class="number">196</span>张图片,耗时:<span class="number">3.10</span>s</span><br></pre></td></tr></table></figure>
<p>增大线程数，输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">线程数:<span class="number">50</span>,耗时:<span class="number">3.10</span>s</span><br><span class="line">线程数:<span class="number">60</span>,耗时:<span class="number">3.07</span>s</span><br><span class="line">线程数:<span class="number">70</span>,耗时:<span class="number">2.50</span>s</span><br><span class="line">线程数:<span class="number">80</span>,耗时:<span class="number">2.31</span>s</span><br><span class="line">线程数:<span class="number">120</span>,耗时:<span class="number">3.67</span>s</span><br></pre></td></tr></table></figure>
<p>可以看到，线程可以有效的提高爬取效率，缩短爬取时间，但必须是一个合理的线程数，越多有时并不是越好的，一般是几十到几百个之间，数值比多进程进程数大许多。</p>
<h3 id="异步协程爬虫"><a href="#异步协程爬虫" class="headerlink" title="异步协程爬虫"></a>异步协程爬虫</h3><p>Python3.5引入了async/await 异步协程语法。详见<a target="_blank" rel="noopener" href="https://www.python.org/dev/peps/pep-0492/">PEP492</a><br>由于asyncio提供了基于socket的异步I/O，支持TCP和UDP协议，但是不支持应用层协议HTTP，所以需要安装异步http请求的aiohttp模块<br>单进程下的异步协程爬虫：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> asyncio <span class="keyword">import</span> Semaphore</span><br><span class="line"><span class="keyword">from</span> aiohttp <span class="keyword">import</span> ClientSession,TCPConnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">session:ClientSession,url:<span class="built_in">str</span>,name:<span class="built_in">str</span>,sem:Semaphore,suffix:<span class="built_in">str</span>=<span class="string">&#x27;jpg&#x27;</span></span>):</span></span><br><span class="line">    path = <span class="string">&#x27;.&#x27;</span>.join([name,suffix])</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> sem:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">            wb_data = <span class="keyword">await</span> response.read()</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(wb_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run_coroutine_crawler</span>(<span class="params">pic_urls:<span class="built_in">list</span>,concurrency:<span class="built_in">int</span></span>):</span></span><br><span class="line">    <span class="comment"># 异步协程爬虫,最大并发请求数concurrency</span></span><br><span class="line">    tasks = []</span><br><span class="line">    sem = Semaphore(concurrency)</span><br><span class="line">    conn =TCPConnector(limit=concurrency)</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> ClientSession(connector=conn) <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pic_urls:</span><br><span class="line">            ts = <span class="built_in">str</span>(<span class="built_in">int</span>(time.time() * <span class="number">10000</span>)) + <span class="built_in">str</span>(random.randint(<span class="number">1</span>, <span class="number">100000</span>))</span><br><span class="line">            tasks.append(asyncio.create_task(download(session,i,ts,sem)))</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line">        end = time.time()</span><br><span class="line">    	print(<span class="string">u&#x27;下载完成,%d张图片,耗时:%.2fs&#x27;</span> % (<span class="built_in">len</span>(pic_urls), (end - start)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coroutine_crawler</span>(<span class="params">concurrency:<span class="built_in">int</span></span>):</span></span><br><span class="line">    pic_urls = get_pic_src(url)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(run_coroutine_crawler(pic_urls,concurrency))</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>执行异步协程爬虫，设置最大并发请求数为100：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coroutine_crawler(<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下载完成,<span class="number">196</span>张图片,耗时:<span class="number">2.27</span>s</span><br></pre></td></tr></table></figure>
<p>可以看出，异步多协程的下载请求效率并不比多线程差，由于磁盘IO读写阻塞，所以还可以进一步优化，使用<a target="_blank" rel="noopener" href="https://pypi.org/project/aiofiles/0.2.1/">aiofiles</a>。<br>针对比较大的多媒体数据下载，异步磁盘IO可以使用aiofiles,以上述例子download可以改为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aiofiles</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">session:ClientSession,url:<span class="built_in">str</span>,name:<span class="built_in">str</span>,sem:Semaphore,suffix:<span class="built_in">str</span>=<span class="string">&#x27;jpg&#x27;</span></span>):</span></span><br><span class="line">    path = <span class="string">&#x27;.&#x27;</span>.join([name,suffix])</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> sem:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">           <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(path,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fd:</span><br><span class="line">            <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">                wb_data_chunk = <span class="keyword">await</span> response.content.read(<span class="number">1024</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> wb_data_chunk:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">await</span> fd.write(wb_data_chunk)</span><br></pre></td></tr></table></figure>
<h3 id="多进程-多线程-爬虫"><a href="#多进程-多线程-爬虫" class="headerlink" title="多进程 + 多线程 爬虫"></a>多进程 + 多线程 爬虫</h3><p>实际采集大量数据的过程中，往往是多种手段来实现爬虫，这样可以充分利用机器CPU，节省采集时间。<br>下面使用多进程（进程数为CPU数，4）+ 多线程 （线程数设为50）来对例子进行更改(上面各个例子导入的模块默认使用):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mixed_process_thread_crawler</span>(<span class="params">processors:<span class="built_in">int</span>,threads:<span class="built_in">int</span></span>):</span></span><br><span class="line">    pool = Pool(processors)</span><br><span class="line">    pic_urls = get_pic_src(url)</span><br><span class="line">    url_groups = allot(pic_urls,processors)</span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> url_groups:</span><br><span class="line">        pool.apply_async(run_multithread_crawler,args=(group,threads))</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure>
<p>执行爬虫：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mixed_process_thread_crawler(<span class="number">4</span>,<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>输出： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.73</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.76</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.76</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.76</span>s</span><br></pre></td></tr></table></figure>
<p>采集时间与异步协程和多线程并无多大的差异，可以使用更大数据量做实验区分。因为多进程+多线程，CPU切换上下文也会造成一定的开销，所以进程数与线程数不能太大，并发请求的时间间隔也要考虑进去。</p>
<h3 id="多进程-异步协程-爬虫"><a href="#多进程-异步协程-爬虫" class="headerlink" title="多进程 + 异步协程 爬虫"></a>多进程 + 异步协程 爬虫</h3><p>使用多进程（进程数为CPU数，4）+ 异步协程（最大并发请求数设为50）来对例子进行更改(上面各个例子导入的模块默认使用):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_coroutine_crawler</span>(<span class="params">pic_urls:<span class="built_in">list</span>,concurrency:<span class="built_in">int</span></span>):</span></span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(run_coroutine_crawler(pic_urls, concurrency))</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mixed_process_coroutine_crawler</span>(<span class="params">processors:<span class="built_in">int</span>,concurrency:<span class="built_in">int</span></span>):</span></span><br><span class="line">    pool = Pool(processors)</span><br><span class="line">    pic_urls = get_pic_src(url)</span><br><span class="line">    url_groups = allot(pic_urls, processors)</span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> url_groups:</span><br><span class="line">        pool.apply_async(_coroutine_crawler, args=(group, concurrency))</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure>
<p>执行爬虫 ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mixed_process_coroutine_crawler(<span class="number">4</span>,<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>输出： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.56</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.54</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.56</span>s</span><br><span class="line">下载完成,<span class="number">49</span>张图片,耗时:<span class="number">2.62</span>s</span><br></pre></td></tr></table></figure>
<p>效果与多进程 + 多线程 爬虫差不多，但是CPU减少了切换线程上下文的开销，而是对每一个协程任务进行监视回调唤醒。使用IO多路复用的底层原理实现。</p>
<h3 id="分布式采集"><a href="#分布式采集" class="headerlink" title="分布式采集"></a>分布式采集</h3><p>关于分布式采集将会单独写一章，使用Map-Reduce+redis来实现分布式爬虫。</p>
<h2 id="轮子们，你们辛苦了"><a href="#轮子们，你们辛苦了" class="headerlink" title="轮子们，你们辛苦了"></a>轮子们，你们辛苦了</h2><p>现实生活中的爬虫不止上面那些，但是基本的骨架是一样的，对于特定的网站需要制定特定的采集规则，所以通用的数据采集爬虫很难实现。所以针对某个网站的数据采集爬虫是需要定制的，但是在不同之中包含着许多的相同、重复性的过程，比如说采集流程，或者对请求头部的伪造，数据持久化的处理等，采集框架应运而生。Scrapy就是目前比较成熟的一个爬虫框架。它可以帮助我们大大减少重复性的代码编写，可以更好的组织采集流程。而我们只需要喝一杯咖啡，编写自己的采集规则，让Scrapy去给我们管理各种各样的爬虫，做些累活。如果你是一个爬虫爱好者，那么scrapy是你的不错选择。由于好奇scrapy的实现流程，所以我才开始打开他的源码学习。<br>有些人觉得scrapy太重，他的爬虫只需要简单的采集，自己写一下就可以搞定了。但如果是大量的爬虫采集呢？怎么去管理这些爬虫呢？怎样才能提高采集效率呀？<br>Scrapy helps！！</p>
<p>另外还有另一个Python采集框架：pyspider。国人编写的，cool<br>感谢轮子们的父母，还有那些辛苦工作的轮子们，你们辛苦了</p>

      </div>
      <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="http://www.1991.site">linkin</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="http://www.1991.site/2020/11/16/%E5%85%B3%E4%BA%8EPython%E7%88%AC%E8%99%AB%E7%A7%8D%E7%B1%BB%E3%80%81%E6%B3%95%E5%BE%8B%E3%80%81%E8%BD%AE%E5%AD%90%E7%9A%84%E4%B8%80%E4%BA%8C%E4%B8%89/">http://www.1991.site/2020/11/16/%E5%85%B3%E4%BA%8EPython%E7%88%AC%E8%99%AB%E7%A7%8D%E7%B1%BB%E3%80%81%E6%B3%95%E5%BE%8B%E3%80%81%E8%BD%AE%E5%AD%90%E7%9A%84%E4%B8%80%E4%BA%8C%E4%B8%89/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/python%E7%88%AC%E8%99%AB/">python爬虫</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2020/11/20/%E5%8F%8D%E7%BC%96%E8%AF%91PC%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">反编译PC微信小程序</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"><div id="gitalk-container"></div>
    </div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:crawler@88.com" class="iconfont icon-email" title="email"></a>
        </div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright">
  <span id="busuanzi_container_site_uv">
    uv:<span id="busuanzi_value_site_uv"></span>
  </span>
  <span id="busuanzi_container_site_pv">
    pv:<span id="busuanzi_value_site_pv"></span>
</span>

  <span class="copyright-year">&copy;2020 - 2021<span class="heart">
      <i class="iconfont icon-cup">|</i>
    </span>
    <span class="author"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn">浙ICP备18022544号-3</a></span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"/>


<script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script>

<script>
  var gitalk = new Gitalk({
    clientID: 'e27dcb1bc0afcd647c41',
    clientSecret: '9972a7d4945d2d2ff7a941e0b75891f567c474f8',
    repo: 'Ih4cker.github.io',
    owner: 'Ih4cker',
    admin: ['Ih4cker'],
    id: md5(location.pathname),
    
      language: 'zh-CN',
    
    distractionFreeMode: 'true'
  });
  gitalk.render('gitalk-container');
</script><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>
